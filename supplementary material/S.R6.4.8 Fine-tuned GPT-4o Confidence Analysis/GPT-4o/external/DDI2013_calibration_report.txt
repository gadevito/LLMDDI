================================================================================
CONFIDENCE CALIBRATION ANALYSIS
Dataset: DDI2013
================================================================================

PREDICTION OUTCOMES
--------------------------------------------------------------------------------
TP:   67 ( 45.3%)
TN:   64 ( 43.2%)
FP:   10 (  6.8%)
FN:    7 (  4.7%)
Total:  148

CALIBRATION ANALYSIS
--------------------------------------------------------------------------------
Correct predictions (n=131):
  Mean confidence:  0.9939 (SD: 0.0470)
  Median:           0.9999

Incorrect predictions (n=17):
  Mean confidence:  0.9638 (SD: 0.1060)
  Median:           0.9976

Statistical comparison (Correct vs Incorrect):
  Test: Mann-Whitney U (one-sided: correct > incorrect)
  U statistic: 1899.00
  P-value: 1.18e-06
  Effect size (rank-biserial r): +0.705
  Interpretation: large
  Significant at α=0.001: ✓ Yes

TRUE POSITIVES vs FALSE POSITIVES (Critical for Label Noise)
--------------------------------------------------------------------------------
True Positives (n=67):
  Mean:  0.9986 (SD: 0.0042)

False Positives (n=10):
  Mean:  0.9517 (SD: 0.1371)

Statistical comparison (TP vs FP):
  Test: Mann-Whitney U (one-sided: TP > FP)
  P-value: 1.93e-02
  Effect size (r): +0.409 (medium)
  Significant at α=0.001: ✗ No

================================================================================
SUGGESTED TEXT FOR PAPER
================================================================================

To verify that high sensitivity reflects genuine discriminative ability rather
than uniform positive bias, we analyzed GPT-4o's prediction confidence via
log-probabilities on the DDI2013. Correct predictions exhibited
significantly higher confidence (mean logprob: 0.99, SD: 0.05)
than incorrect predictions (mean: 0.96, SD: 0.11);
Mann-Whitney U test, p < 0.001, effect size r = +0.71
(large effect).

Critically, true positive predictions showed higher confidence
(mean logprob: 1.00) than false positive predictions
(mean: 0.95). This calibration
pattern indicates the model assigns high confidence selectively to genuine
interactions rather than indiscriminately to all positive predictions.

