================================================================================
CONFIDENCE CALIBRATION ANALYSIS
Dataset: DDI2011
================================================================================

PREDICTION OUTCOMES
--------------------------------------------------------------------------------
TP:   23 ( 35.9%)
TN:   28 ( 43.8%)
FP:    4 (  6.2%)
FN:    9 ( 14.1%)
Total:   64

CALIBRATION ANALYSIS
--------------------------------------------------------------------------------
Correct predictions (n=51):
  Mean confidence:  0.9864 (SD: 0.0810)
  Median:           1.0000

Incorrect predictions (n=13):
  Mean confidence:  0.9455 (SD: 0.1071)
  Median:           0.9855

Statistical comparison (Correct vs Incorrect):
  Test: Mann-Whitney U (one-sided: correct > incorrect)
  U statistic: 592.00
  P-value: 7.17e-06
  Effect size (rank-biserial r): +0.786
  Interpretation: large
  Significant at α=0.001: ✓ Yes

TRUE POSITIVES vs FALSE POSITIVES (Critical for Label Noise)
--------------------------------------------------------------------------------
True Positives (n=23):
  Mean:  0.9954 (SD: 0.0198)

False Positives (n=4):
  Mean:  0.9536 (SD: 0.0685)

================================================================================
SUGGESTED TEXT FOR PAPER
================================================================================

To verify that high sensitivity reflects genuine discriminative ability rather
than uniform positive bias, we analyzed GPT-4o's prediction confidence via
log-probabilities on the DDI2011. Correct predictions exhibited
significantly higher confidence (mean logprob: 0.99, SD: 0.08)
than incorrect predictions (mean: 0.95, SD: 0.11);
Mann-Whitney U test, p < 0.001, effect size r = +0.79
(large effect).

