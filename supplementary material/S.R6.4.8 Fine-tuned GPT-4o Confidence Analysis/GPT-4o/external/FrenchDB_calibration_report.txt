================================================================================
CONFIDENCE CALIBRATION ANALYSIS
Dataset: FrenchDB
================================================================================

PREDICTION OUTCOMES
--------------------------------------------------------------------------------
TP: 4199 ( 48.9%)
TN: 3917 ( 45.6%)
FP:  380 (  4.4%)
FN:   98 (  1.1%)
Total: 8594

CALIBRATION ANALYSIS
--------------------------------------------------------------------------------
Correct predictions (n=8116):
  Mean confidence:  0.9921 (SD: 0.0543)
  Median:           0.9999

Incorrect predictions (n=478):
  Mean confidence:  0.8871 (SD: 0.2041)
  Median:           0.9890

Statistical comparison (Correct vs Incorrect):
  Test: Mann-Whitney U (one-sided: correct > incorrect)
  U statistic: 3335368.00
  P-value: 9.09e-155
  Effect size (rank-biserial r): +0.720
  Interpretation: large
  Significant at α=0.001: ✓ Yes

TRUE POSITIVES vs FALSE POSITIVES (Critical for Label Noise)
--------------------------------------------------------------------------------
True Positives (n=4199):
  Mean:  0.9895 (SD: 0.0651)

False Positives (n=380):
  Mean:  0.9089 (SD: 0.1942)

Statistical comparison (TP vs FP):
  Test: Mann-Whitney U (one-sided: TP > FP)
  P-value: 2.24e-64
  Effect size (r): +0.523 (large)
  Significant at α=0.001: ✓ Yes

================================================================================
SUGGESTED TEXT FOR PAPER
================================================================================

To verify that high sensitivity reflects genuine discriminative ability rather
than uniform positive bias, we analyzed GPT-4o's prediction confidence via
log-probabilities on the FrenchDB. Correct predictions exhibited
significantly higher confidence (mean logprob: 0.99, SD: 0.05)
than incorrect predictions (mean: 0.89, SD: 0.20);
Mann-Whitney U test, p < 0.001, effect size r = +0.72
(large effect).

Critically, true positive predictions showed higher confidence
(mean logprob: 0.99) than false positive predictions
(mean: 0.91), with the difference
being statistically significant (Mann-Whitney U, p < 0.001,
r = +0.52, large effect). This calibration
pattern indicates the model assigns high confidence selectively to genuine
interactions rather than indiscriminately to all positive predictions.

