optimization:
  n_trials: 1000
  study_name: "lora_hyperparameter_optimization"
  direction: "minimize"

lora_keys: ["self_attn.o_proj", "self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", "mlp.gate_proj", "mlp.up_proj", "mlp.down_proj"]

parameter_space:
  learning_rate:
    type: "categorical"
    choices: [1e-5, 5e-5, 1e-4, 2e-4, 3e-4, 5e-4]
  
  batch_size:
    type: "categorical"
    choices: [8] 
  
  num_layers:
    type: "categorical"
    choices: [16, 24, 28]
  
  lora_alpha_scaling:  
    type: "categorical"
    choices: [1, 2]  # 1 for alpha=rank, 2 for alpha=2*rank

  lora_rank:
    type: "categorical"
    choices: [16, 32]  
  
  lora_dropout:
    type: "float"
    low: 0.0
    high: 0.03
    step: 0.01
  
  lora_scale:
    type: "float"
    low: 4.0
    high: 4.3
    step: 0.1