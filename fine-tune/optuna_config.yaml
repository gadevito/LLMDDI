optimization:
  n_trials: 1000
  study_name: "lora_hyperparameter_optimization"
  direction: "minimize"

lora_keys: ["self_attn.o_proj", "self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", "mlp.gate_proj", "mlp.up_proj", "mlp.down_proj"]

parameter_space:
  learning_rate:
    type: "float"
    low: 1.8e-4
    high: 2.8e-4
    log: true
  
  batch_size:
    type: "categorical"
    choices: [8] # we can also try 4
  
  num_layers:
    type: "categorical"
    choices: [16, 18, 20, 22, 24, 26, 28]
  
  lora_alpha_scaling:  
    type: "categorical"
    choices: [1, 2]  # 1 for alpha=rank, 2 for alpha=2*rank

  lora_rank:
    type: "categorical"
    choices: [16, 32]  
  
  lora_dropout:
    type: "float"
    low: 0.0
    high: 0.02
    step: 0.001
  
  lora_scale:
    type: "float"
    low: 3.8
    high: 4.4
    step: 0.1